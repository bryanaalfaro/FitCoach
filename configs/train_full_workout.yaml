model:
    llama2_7b_path: './Llama-2-7b-hf'
    kwargs:
        checkpoint_path: './ckpts_streamvlm/ckpts_streamvlm/state_dict.pth.tar'
        strict_checkpoint: False
        bf16: True
        xattn_config:
            xattn_type: 'dotprod'
            xattn_block_size: 1
            adapter_insert_layers: "[6, 8, 10, 12, 14, 16, 18, 20, 22]"
            num_of_xattn_heads: 1
        trainable:
            llm: True
            xattn: False
            vision: False
        peft_config:
            r: 32
            lora_alpha: 16
            lora_dropout: 0.05
            target_modules:
                - 'q_proj'
                - 'v_proj'
                - 'k_proj'
                - 'lm_head'

datasets:
    train:
        name: 'fitcoach-dataset'
        kwargs:
            data_root: './data'
            split: 'train'
            eval_mode: 'full_workout_sliding_window'
            shuffle_videos: False
            # sliding_window_length: 15

training:
    output_dir: './ckpts_full_workout'
    batch_size: 1
    num_epochs: 1
    max_steps:
    lr: 5.0e-6
    weight_decay: 0.01
    gradient_accumulation_steps: 1
    max_grad_norm: 1.0
    num_workers: 0
    log_every: 10
    save_every: 500
    max_sequence_length: 4096


evaluator:
    sampling_kwargs:
        do_sample: False # Greedy decoding
        temperature: 0. # Ignored
        max_feedback_length: 128
        feats_frequency: 4 # num of features per second
